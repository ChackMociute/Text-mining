{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;*The first two sentences are marked very intuitively. VADER considers 'love' a positive word and its negation then as negative. The third sentence is considered more positive than the first because VADER takes the smiley face into account.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Fourth sentece is similar to the first, 'ruins' is a negative word and thusly the sentece is negative. The fifth sentence is again a negation of the fourth one and so is positive.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The last two senteces contain the words 'lies' and 'like' which have a negative and positive sentiment respectively in certain contexts. Therefore, even though they are neutral in these sentences, VADER labels them with their respective sentiments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "from random import sample\n",
    "\n",
    "with open('favorite-tweets.jsonl', 'r') as jf:\n",
    "    tweets = [json.loads(i) for i in list(jf)]\n",
    "\n",
    "with open('my_tweets.json', 'w') as jf:\n",
    "    jf.write(json.dumps({i: {\"sentiment_label\": \"\",\n",
    "         \"text_of_tweet\": t['Text'],\n",
    "         \"tweet_url\": t['LinkToTweet']}\n",
    "     for i, t in enumerate(sample(tweets, 50))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'sentiment_label': 'negative', 'text_of_tweet': \"@IamNotThatAlex The infamous doxxing website named after a bird. If that's not enough, count yourself lucky and stay away \", 'tweet_url': 'https://twitter.com/im_just_laur/status/1553870749290713089'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = vader_model.polarity_scores(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.30      0.41        23\n",
      "     neutral       0.42      0.56      0.48         9\n",
      "    positive       0.48      0.72      0.58        18\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.51      0.53      0.49        50\n",
      "weighted avg       0.54      0.50      0.48        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the classification report we can observer that the tweets with the negative label have a high precision and low(er) recall, and the tweets with the neutral and positive label have a lower precision, and higher recall. This indicates that the VADER outputs labeled less tweets with negative, compared to the (23) gold negative tweets. Similarly, VADER labeled more tweets as neutral and positive than there truely were in the gold set (9 and 18 respectivley). The f1-scores indicate that VADER was most succesfull for labeling positive tweets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: NEUTRAL\n",
      "Tweet 4: My favorite Stable Genius. https://t.co/PsNNKjlXb3\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "Tweet 10: More #SCOTUS/OSG news. Brian Fletcher was a clerk to now-Attorney General Merrick Garland on the D.C. Circuit and to Justice Ruth Bader Ginsburg. https://t.co/ZHFV0xCY3P\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "Tweet 26: @mims This cuts both ways. My wife's idea for national service is that every person be forced to live and work for two years in a region of the country distinctly separate from their own in terms of economy and culture.\n",
      "gold: neutral || vader: negative\n",
      "\n",
      "Tweet 36: I'm really hitting the \"wish for cosmic justice\" phase of this pandemic. Particularly the kind of justice that comes from the threefold law.\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "\n",
      "LABEL: POSITIVE\n",
      "Tweet 2: @A2Lintra @PEMatson @NewfieldsToday it closed real early, so we didn't even quite finish the museum much less explore any of the grounds. So, there is a lot more to see on a future visit.\n",
      "gold: positive || vader: neutral\n",
      "\n",
      "Tweet 6: @KMNetter @cathleendecker @pkcapitol She was on MSNBC while it was going on. She was hardly cowering in fear and afraid to let anyone in. The lady sounded both outraged and ready to kick ass to me. Rightfully so.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "Tweet 17: Just as you feel when you look on the river and sky, so I felt; Just as any of you is one of a living crowd, I was one of a crowd  --Walt Whitman, whose 200th birthday is Friday #Whitman200 https://t.co/yFdTwklH9h\n",
      "gold: positive || vader: neutral\n",
      "\n",
      "Tweet 35: When Covid struck Michigan hard @GovWhitmer listened to science &amp; saved thousands of lives with her swift action.  But make no mistake, the pathogens of hate and division spread, incited by the president and his complicit supporters.   We got your back #BigGretch.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "Tweet 37: White people: watch the Starbucks arrest video.  See the white folks arguing with the police and asking why two innocent black men were being arrested?  THIS is how you use your privilege.  Because if one of us had said something we'd get arrested too.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "\n",
      "LABEL: NEGATIVE\n",
      "Tweet 0: @IamNotThatAlex The infamous doxxing website named after a bird. If that's not enough, count yourself lucky and stay away \n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 3: @Scotty1872 It's very easy to say that when you're in the group that is well represented.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 5: Nationalize the cruise lines, turn the ships into hospitals. https://t.co/AleE3BFqDa\n",
      "gold: negative || vader: neutral\n",
      "\n",
      "Tweet 9: @realkatiejow @AOC congrats on conflating sociopathy with patriotism\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 13: 4.Condemn and censure our colleague who wore a confederate flag on his face to session on Friday, 04.24.2020. In fact, you can also pass the legislation that prohibits white supremacist symbols from the Capitol completely. 6/x\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 20: I went to Monticello last year and it struck with me is that even saying The Founding Fathers were geniuses who also owned slaves is wrong. They are woven together. The freedom they had for academic pursuits was one afforded to them by human capital https://t.co/rwSWCXns3o\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 21: Time that women spend asking and reminding their male partners to do housework, and time women spend praising and rewarding their male partners when they do housework, is time those women themselves are spending on housework.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 22: If you were an alien showing up on Earth for the first time in the last 24 hours and all you had to go on was what you read on Twitter, you would probably thinking the thing Charlie Watts was most famous for was punching Mick Jagger in the face.\n",
      "gold: negative || vader: neutral\n",
      "\n",
      "Tweet 30: You allowed cops to utilize your studio lot yesterday so they could get their teargas and rubber bullets ready. They set up base there. Make a statement on that.  All of us, especially those of us in your industry, want to know why https://t.co/aguVw1G6dO\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 31: I felt so optimistic at the start of this week about my plan to spend less time on Twitter, but the intervening months have tested my resolve.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print 10 tweets that were misclassified\n",
    "for label in set(gold):\n",
    "    print('LABEL:', label.upper())\n",
    "    c = 0\n",
    "    for i in range(len(tweets)):\n",
    "        if gold[i] == label and all_vader_output[i] != label:\n",
    "            print(f\"Tweet {i}: {tweets[i]}\")\n",
    "            print('gold:', gold[i], '|| vader:', all_vader_output[i])\n",
    "            print()\n",
    "            c += 1\n",
    "        if c == 10:\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEUTRAL LABELS:\n",
    "*The neutral tweets that were labeled incorrectly by VADER, generally seem to do so as a result of having positive or negative words in the sentence, that do not necessarily contribute to the sentiment. Examples of the positive words can be seen in tweets 4, 10 and 26, with words such as 'favorate' and 'justice'. The only tweet labeled as negative (tweet 26) contains negative words such as 'forced' and 'cut'.*\n",
    "\n",
    "#### POSITIVE LABELS:\n",
    "*All misclasifications for the tweets that should have been marked as positive, are because VADER marked them as neutral instead. Most of these tweets state something positive, despite something negative (2, 6, 35, 37). These generally contain a mix of positive and negative words, which makes it understandable that it was classified as neutral. More complex reasoning is required to understand the true meaning/sentement of such statements. Additionally, one of the tweets contained a metaphorical statement (17), which is also inherently difficult to classify correctly.*\n",
    "\n",
    "#### NEGATIVE LABELS:\n",
    "*Many of the negative tweets, VADER has labeled as positive instead. This is partially due to people using positive (or strengthening) words to describe negative things, to emphasise how bad it is (0, 3, 20, 21, 22, 30, 31). In some cases, it is even done sarcastically (9). Some tweets that VADER marked as neutral contain a more cryptic negative sentement, and does not contain (many strong) negative or positive words (5, 22).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#Vader\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') # 'en_core_web_sm'\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Load data from airlinetweets folder \n",
    "import pathlib\n",
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "#airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "airline_tweets_train = load_files('airlinetweets')\n",
    "\n",
    "len(airline_tweets_train.data)\n",
    "#print(airline_tweets_train.data)\n",
    "print(type(airline_tweets_train.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------MIN_DF=2-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.91      0.87       353\n",
      "           1       0.86      0.73      0.79       322\n",
      "           2       0.80      0.87      0.83       276\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.84      0.83      0.83       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       327\n",
      "           1       0.84      0.68      0.75       311\n",
      "           2       0.83      0.87      0.85       313\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.82      0.82       951\n",
      "\n",
      "\n",
      "-------------------------MIN_DF=5-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.86       321\n",
      "           1       0.86      0.73      0.79       319\n",
      "           2       0.86      0.87      0.86       311\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86       345\n",
      "           1       0.87      0.75      0.81       321\n",
      "           2       0.84      0.87      0.86       285\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.85      0.84      0.84       951\n",
      "weighted avg       0.85      0.84      0.84       951\n",
      "\n",
      "\n",
      "-------------------------MIN_DF=10-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.86       350\n",
      "           1       0.78      0.75      0.76       295\n",
      "           2       0.87      0.81      0.84       306\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       339\n",
      "           1       0.81      0.70      0.75       300\n",
      "           2       0.86      0.84      0.85       312\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for min_df in [2, 5, 10]:\n",
    "    # Create feature extractors\n",
    "    airline_vec = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    # Extract features\n",
    "    airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "    \n",
    "    # Split train and test data\n",
    "    tfidf_train, tfidf_test, tfidf_y_train, tfidf_y_test = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "    count_train, count_test, count_y_train, count_y_test = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "    \n",
    "    # Train and predict\n",
    "    print(f\"-------------------------MIN_DF={min_df}-------------------------\")\n",
    "    print(\"Trained with tfidf:\")\n",
    "    print(classification_report(tfidf_y_test, MultinomialNB().fit(tfidf_train, tfidf_y_train).predict(tfidf_test)), '\\n')\n",
    "    print(\"Trained with bow:\")\n",
    "    print(classification_report(count_y_test, MultinomialNB().fit(count_train, count_y_train).predict(count_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airline_vec = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "data, _, labels, _ = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "clf = MultinomialNB().fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1506.0 @\n",
      "0 1391.0 united\n",
      "0 1227.0 .\n",
      "0 428.0 ``\n",
      "0 413.0 ?\n",
      "0 406.0 flight\n",
      "0 376.0 !\n",
      "0 314.0 #\n",
      "0 221.0 n't\n",
      "0 167.0 ''\n",
      "0 128.0 's\n",
      "0 115.0 :\n",
      "0 114.0 service\n",
      "0 104.0 virginamerica\n",
      "0 96.0 cancelled\n",
      "0 93.0 delayed\n",
      "0 91.0 get\n",
      "0 89.0 customer\n",
      "0 88.0 bag\n",
      "0 86.0 time\n",
      "0 84.0 plane\n",
      "0 78.0 ...\n",
      "0 73.0 hours\n",
      "0 72.0 http\n",
      "0 71.0 ;\n",
      "0 71.0 -\n",
      "0 68.0 'm\n",
      "0 67.0 hour\n",
      "0 65.0 gate\n",
      "0 64.0 &\n",
      "0 63.0 late\n",
      "0 63.0 airline\n",
      "0 60.0 still\n",
      "0 56.0 flights\n",
      "0 56.0 amp\n",
      "0 55.0 help\n",
      "0 55.0 2\n",
      "0 53.0 worst\n",
      "0 53.0 ca\n",
      "0 52.0 delay\n",
      "0 51.0 would\n",
      "0 49.0 one\n",
      "0 49.0 never\n",
      "0 47.0 like\n",
      "0 47.0 flightled\n",
      "0 47.0 $\n",
      "0 46.0 've\n",
      "0 43.0 (\n",
      "0 42.0 waiting\n",
      "0 42.0 us\n",
      "0 42.0 lost\n",
      "0 41.0 ever\n",
      "0 41.0 3\n",
      "0 40.0 back\n",
      "0 40.0 )\n",
      "0 39.0 day\n",
      "0 38.0 wait\n",
      "0 38.0 fly\n",
      "0 38.0 due\n",
      "0 37.0 u\n",
      "0 35.0 thanks\n",
      "0 35.0 people\n",
      "0 35.0 luggage\n",
      "0 35.0 check\n",
      "0 35.0 airport\n",
      "0 34.0 hold\n",
      "0 33.0 seat\n",
      "0 33.0 really\n",
      "0 32.0 last\n",
      "0 31.0 ticket\n",
      "0 31.0 terrible\n",
      "0 31.0 seats\n",
      "0 31.0 bags\n",
      "0 31.0 another\n",
      "0 30.0 even\n",
      "0 29.0 problems\n",
      "0 29.0 got\n",
      "0 29.0 baggage\n",
      "0 28.0 trying\n",
      "0 28.0 told\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1385.0 @\n",
      "1 515.0 ?\n",
      "1 504.0 .\n",
      "1 301.0 jetblue\n",
      "1 284.0 :\n",
      "1 267.0 southwestair\n",
      "1 265.0 united\n",
      "1 240.0 #\n",
      "1 236.0 ``\n",
      "1 222.0 flight\n",
      "1 179.0 americanair\n",
      "1 174.0 http\n",
      "1 168.0 !\n",
      "1 160.0 usairways\n",
      "1 130.0 's\n",
      "1 79.0 -\n",
      "1 78.0 get\n",
      "1 71.0 virginamerica\n",
      "1 69.0 flights\n",
      "1 65.0 please\n",
      "1 63.0 )\n",
      "1 61.0 help\n",
      "1 58.0 ''\n",
      "1 56.0 (\n",
      "1 51.0 ...\n",
      "1 49.0 need\n",
      "1 46.0 n't\n",
      "1 46.0 dm\n",
      "1 44.0 ;\n",
      "1 40.0 “\n",
      "1 40.0 would\n",
      "1 37.0 ”\n",
      "1 37.0 us\n",
      "1 37.0 &\n",
      "1 36.0 way\n",
      "1 36.0 flying\n",
      "1 36.0 fleet\n",
      "1 36.0 fleek\n",
      "1 35.0 know\n",
      "1 35.0 change\n",
      "1 34.0 tomorrow\n",
      "1 34.0 thanks\n",
      "1 33.0 hi\n",
      "1 31.0 'm\n",
      "1 29.0 fly\n",
      "1 29.0 cancelled\n",
      "1 28.0 one\n",
      "1 28.0 new\n",
      "1 28.0 like\n",
      "1 27.0 could\n",
      "1 27.0 amp\n",
      "1 26.0 today\n",
      "1 26.0 number\n",
      "1 25.0 time\n",
      "1 24.0 check\n",
      "1 24.0 airport\n",
      "1 23.0 see\n",
      "1 23.0 go\n",
      "1 21.0 travel\n",
      "1 21.0 tickets\n",
      "1 21.0 morning\n",
      "1 21.0 make\n",
      "1 21.0 going\n",
      "1 21.0 destinationdragons\n",
      "1 21.0 back\n",
      "1 20.0 use\n",
      "1 20.0 sent\n",
      "1 20.0 reservation\n",
      "1 20.0 guys\n",
      "1 19.0 ticket\n",
      "1 19.0 start\n",
      "1 19.0 seat\n",
      "1 19.0 rt\n",
      "1 19.0 next\n",
      "1 19.0 first\n",
      "1 19.0 2\n",
      "1 18.0 want\n",
      "1 18.0 trying\n",
      "1 18.0 still\n",
      "1 18.0 question\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1329.0 @\n",
      "2 1029.0 !\n",
      "2 766.0 .\n",
      "2 319.0 #\n",
      "2 299.0 southwestair\n",
      "2 294.0 jetblue\n",
      "2 281.0 thanks\n",
      "2 254.0 thank\n",
      "2 248.0 united\n",
      "2 232.0 ``\n",
      "2 183.0 flight\n",
      "2 172.0 :\n",
      "2 166.0 americanair\n",
      "2 144.0 usairways\n",
      "2 131.0 great\n",
      "2 94.0 )\n",
      "2 91.0 service\n",
      "2 80.0 virginamerica\n",
      "2 73.0 best\n",
      "2 72.0 love\n",
      "2 72.0 http\n",
      "2 68.0 customer\n",
      "2 63.0 much\n",
      "2 62.0 guys\n",
      "2 60.0 ;\n",
      "2 57.0 's\n",
      "2 56.0 awesome\n",
      "2 52.0 -\n",
      "2 45.0 time\n",
      "2 45.0 got\n",
      "2 44.0 good\n",
      "2 44.0 airline\n",
      "2 43.0 &\n",
      "2 42.0 get\n",
      "2 41.0 amazing\n",
      "2 39.0 today\n",
      "2 38.0 n't\n",
      "2 38.0 help\n",
      "2 38.0 crew\n",
      "2 36.0 amp\n",
      "2 34.0 flying\n",
      "2 33.0 appreciate\n",
      "2 31.0 made\n",
      "2 30.0 us\n",
      "2 30.0 fly\n",
      "2 30.0 ...\n",
      "2 28.0 response\n",
      "2 28.0 home\n",
      "2 28.0 gate\n",
      "2 27.0 nice\n",
      "2 27.0 ever\n",
      "2 27.0 back\n",
      "2 27.0 ''\n",
      "2 25.0 well\n",
      "2 24.0 see\n",
      "2 24.0 flights\n",
      "2 23.0 u\n",
      "2 23.0 staff\n",
      "2 23.0 ?\n",
      "2 23.0 (\n",
      "2 23.0 're\n",
      "2 22.0 'm\n",
      "2 22.0 'll\n",
      "2 21.0 work\n",
      "2 21.0 like\n",
      "2 21.0 know\n",
      "2 21.0 job\n",
      "2 21.0 helpful\n",
      "2 21.0 day\n",
      "2 20.0 would\n",
      "2 20.0 way\n",
      "2 20.0 team\n",
      "2 20.0 really\n",
      "2 20.0 please\n",
      "2 20.0 new\n",
      "2 20.0 follow\n",
      "2 20.0 first\n",
      "2 19.0 wait\n",
      "2 19.0 southwest\n",
      "2 19.0 finally\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer, classifier, n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
