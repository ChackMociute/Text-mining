{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;*The first two sentences are marked very intuitively. VADER considers 'love' a positive word and its negation then as negative. The third sentence is considered more positive than the first because VADER takes the smiley face into account.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Fourth sentece is similar to the first, 'ruins' is a negative word and thusly the sentece is negative. The fifth sentence is again a negation of the fourth one and so is positive.\\\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The last two senteces contain the words 'lies' and 'like' which have a negative and positive sentiment respectively in certain contexts. Therefore, even though they are neutral in these sentences, VADER labels them with their respective sentiments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'sentiment_label': 'negative', 'text_of_tweet': \"@IamNotThatAlex The infamous doxxing website named after a bird. If that's not enough, count yourself lucky and stay away \", 'tweet_url': 'https://twitter.com/im_just_laur/status/1553870749290713089'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = vader_model.polarity_scores(the_tweet)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.64      0.30      0.41        23\n",
      "     neutral       0.42      0.56      0.48         9\n",
      "    positive       0.48      0.72      0.58        18\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.51      0.53      0.49        50\n",
      "weighted avg       0.54      0.50      0.48        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*From the classification report we can observer that the tweets with the negative label have a high precision and lower recall, and the tweets with the neutral and positive label have a lower precision, and higher recall. This indicates that VADER outputs negatively labeled tweets less compared to the (23) gold negative tweet labels. Similarly, VADER labeled more tweets as neutral and positive than there actually were in the gold set (9 and 18 respectivley). The f1-scores indicate that VADER was most succesfull for labeling positive tweets.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: POSITIVE\n",
      "Tweet 2: @A2Lintra @PEMatson @NewfieldsToday it closed real early, so we didn't even quite finish the museum much less explore any of the grounds. So, there is a lot more to see on a future visit.\n",
      "gold: positive || vader: neutral\n",
      "\n",
      "Tweet 6: @KMNetter @cathleendecker @pkcapitol She was on MSNBC while it was going on. She was hardly cowering in fear and afraid to let anyone in. The lady sounded both outraged and ready to kick ass to me. Rightfully so.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "Tweet 17: Just as you feel when you look on the river and sky, so I felt; Just as any of you is one of a living crowd, I was one of a crowd  --Walt Whitman, whose 200th birthday is Friday #Whitman200 https://t.co/yFdTwklH9h\n",
      "gold: positive || vader: neutral\n",
      "\n",
      "Tweet 35: When Covid struck Michigan hard @GovWhitmer listened to science &amp; saved thousands of lives with her swift action.  But make no mistake, the pathogens of hate and division spread, incited by the president and his complicit supporters.   We got your back #BigGretch.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "Tweet 37: White people: watch the Starbucks arrest video.  See the white folks arguing with the police and asking why two innocent black men were being arrested?  THIS is how you use your privilege.  Because if one of us had said something we'd get arrested too.\n",
      "gold: positive || vader: negative\n",
      "\n",
      "\n",
      "LABEL: NEGATIVE\n",
      "Tweet 0: @IamNotThatAlex The infamous doxxing website named after a bird. If that's not enough, count yourself lucky and stay away \n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 3: @Scotty1872 It's very easy to say that when you're in the group that is well represented.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 5: Nationalize the cruise lines, turn the ships into hospitals. https://t.co/AleE3BFqDa\n",
      "gold: negative || vader: neutral\n",
      "\n",
      "Tweet 9: @realkatiejow @AOC congrats on conflating sociopathy with patriotism\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 13: 4.Condemn and censure our colleague who wore a confederate flag on his face to session on Friday, 04.24.2020. In fact, you can also pass the legislation that prohibits white supremacist symbols from the Capitol completely. 6/x\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 20: I went to Monticello last year and it struck with me is that even saying The Founding Fathers were geniuses who also owned slaves is wrong. They are woven together. The freedom they had for academic pursuits was one afforded to them by human capital https://t.co/rwSWCXns3o\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 21: Time that women spend asking and reminding their male partners to do housework, and time women spend praising and rewarding their male partners when they do housework, is time those women themselves are spending on housework.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 22: If you were an alien showing up on Earth for the first time in the last 24 hours and all you had to go on was what you read on Twitter, you would probably thinking the thing Charlie Watts was most famous for was punching Mick Jagger in the face.\n",
      "gold: negative || vader: neutral\n",
      "\n",
      "Tweet 30: You allowed cops to utilize your studio lot yesterday so they could get their teargas and rubber bullets ready. They set up base there. Make a statement on that.  All of us, especially those of us in your industry, want to know why https://t.co/aguVw1G6dO\n",
      "gold: negative || vader: positive\n",
      "\n",
      "Tweet 31: I felt so optimistic at the start of this week about my plan to spend less time on Twitter, but the intervening months have tested my resolve.\n",
      "gold: negative || vader: positive\n",
      "\n",
      "\n",
      "LABEL: NEUTRAL\n",
      "Tweet 4: My favorite Stable Genius. https://t.co/PsNNKjlXb3\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "Tweet 10: More #SCOTUS/OSG news. Brian Fletcher was a clerk to now-Attorney General Merrick Garland on the D.C. Circuit and to Justice Ruth Bader Ginsburg. https://t.co/ZHFV0xCY3P\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "Tweet 26: @mims This cuts both ways. My wife's idea for national service is that every person be forced to live and work for two years in a region of the country distinctly separate from their own in terms of economy and culture.\n",
      "gold: neutral || vader: negative\n",
      "\n",
      "Tweet 36: I'm really hitting the \"wish for cosmic justice\" phase of this pandemic. Particularly the kind of justice that comes from the threefold law.\n",
      "gold: neutral || vader: positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print 10 tweets that were misclassified\n",
    "for label in set(gold):\n",
    "    print('LABEL:', label.upper())\n",
    "    c = 0\n",
    "    for i in range(len(tweets)):\n",
    "        if gold[i] == label and all_vader_output[i] != label:\n",
    "            print(f\"Tweet {i}: {tweets[i]}\")\n",
    "            print('gold:', gold[i], '|| vader:', all_vader_output[i])\n",
    "            print()\n",
    "            c += 1\n",
    "        if c == 10:\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NEUTRAL LABELS:\n",
    "*The neutral tweets that were labeled incorrectly by VADER, generally seem to do so as a result of having positive or negative words in the sentence, that do not necessarily contribute to the sentiment. Examples of the positive words can be seen in tweets 4, 10 and 26, with words such as 'favorate' and 'justice'. The only tweet labeled as negative (tweet 26) contains negative words such as 'forced' and 'cut'.*\n",
    "\n",
    "#### POSITIVE LABELS:\n",
    "*All misclasifications for the tweets that should have been marked as positive, are because VADER marked them as neutral instead. Most of these tweets state something positive, despite something negative (2, 6, 35, 37). These generally contain a mix of positive and negative words, which makes it understandable that it was classified as neutral. More complex reasoning is required to understand the true meaning/sentiment of such statements. Additionally, one of the tweets contained a metaphorical statement (17), which is also inherently difficult to classify correctly.*\n",
    "\n",
    "#### NEGATIVE LABELS:\n",
    "*Many of the negative tweets, VADER has labeled as positive instead. This is partially due to people using positive (or strengthening) words to describe negative things, to emphasise how bad it is (0, 3, 20, 21, 22, 30, 31). In some cases, it is even done sarcastically (9). Some tweets that VADER marked as neutral contain a more cryptic negative sentiment, and does not contain (many strong) negative or positive words (5, 22).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, lemmatize=False, parts_of_speech_to_consider=None, verbose=0):\n",
    "    doc = nlp(textual_unit)\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            to_add = token.text\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "                if to_add == '-PRON-': to_add = token.text\n",
    "            if parts_of_speech_to_consider and token.pos_ in parts_of_speech_to_consider:\n",
    "                input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "airline_tweets_train = load_files('airlinetweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.49      0.60      1750\n",
      "     neutral       0.57      0.56      0.56      1515\n",
      "    positive       0.56      0.83      0.67      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.64      0.63      0.61      4755\n",
      "weighted avg       0.65      0.62      0.61      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run vader on tweets\n",
    "all_vader_output = list()\n",
    "for sent in airline_tweets_train.data:\n",
    "    scores = vader_model.polarity_scores(str(sent))\n",
    "    #print()\n",
    "    #print('VADER OUTPUT', scores)\n",
    "    vader_label = vader_output_to_label(scores)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmatized text\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=True)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only adjectives\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=False, parts_of_speech_to_consider={'ADJ'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only adjectives and after having lemmatized the text\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=True, parts_of_speech_to_consider={'ADJ'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only nouns\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=False, parts_of_speech_to_consider={'NOUN'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only nouns and after having lemmatized the text\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=True, parts_of_speech_to_consider={'NOUN'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only verbs\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=False, parts_of_speech_to_consider={'VERB'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only verbs and after having lemmatized the text\n",
    "all_vader_output = list()\n",
    "for i in airline_tweets_train.data:\n",
    "    lemtext = run_vader(str(i), lemmatize=True, parts_of_speech_to_consider={'VERB'}, verbose=0)\n",
    "    vader_label = vader_output_to_label(lemtext)\n",
    "    all_vader_output.append(vader_label)\n",
    "\n",
    "print(classification_report([airline_tweets_train.target_names[i] for i in airline_tweets_train.target], all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparing the classification reports of lemmatized and unlemmatized tweets we see that lemmatization does not really result in a significant difference. For every report the scores differ between 0.01 and 0.05. Lemmatization converts a word to its base form which does not really help VADER with semtiment classification in this case.*\n",
    "\n",
    "*The best performance that we get using only one POS catagory is from the only adjectives report. The f1 score is: neg 0.34, neutr 0.56 and pos 0.53 with an accuracy of 0.50. Specifically the recall is really high (0.89) with a low precision (0.41) for the neutral tweets meaning it returns many results, but most of its predicted labels are incorrect when compared to the training labels. The opposite is true for the negative and positive tweets. There the recall is lower than the precision. VADER predicts the sentiment correctly of a low number of sentences.*\n",
    "\n",
    "*The next best POS category is only using verbs and then nouns as tags. VADER however performs best when all the POS labels are used. The respective performance of f1-scores when all labels are used is: neg 0.60, neutr 0.56 and pos 0.67 with an accuracy of 0.62 which is the highest out of all.*\n",
    "\n",
    "*All POS are helpfull since VADER can use all words with sentiment to figure out the overall sentiment of a sentance. If you only want to use one tag in this model then you are best off by using the adjective tag. However it will always be more accurate when all the tags are used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------MIN_DF=2-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       354\n",
      "           1       0.81      0.69      0.75       298\n",
      "           2       0.81      0.87      0.84       299\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.81      0.81       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       339\n",
      "           1       0.84      0.72      0.78       313\n",
      "           2       0.82      0.85      0.84       299\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.83       951\n",
      "\n",
      "\n",
      "-------------------------MIN_DF=5-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       369\n",
      "           1       0.83      0.71      0.77       278\n",
      "           2       0.85      0.84      0.85       304\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.83      0.83       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.92      0.88       362\n",
      "           1       0.80      0.78      0.79       295\n",
      "           2       0.86      0.79      0.82       294\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n",
      "\n",
      "-------------------------MIN_DF=10-------------------------\n",
      "Trained with tfidf:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       310\n",
      "           1       0.85      0.76      0.80       317\n",
      "           2       0.86      0.82      0.84       324\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      " \n",
      "\n",
      "Trained with bow:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       350\n",
      "           1       0.84      0.74      0.79       317\n",
      "           2       0.83      0.83      0.83       284\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for min_df in [2, 5, 10]:\n",
    "    # Create feature extractors\n",
    "    airline_vec = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    # Extract features\n",
    "    airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "    airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "    \n",
    "    # Split train and test data\n",
    "    tfidf_train, tfidf_test, tfidf_y_train, tfidf_y_test = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "    count_train, count_test, count_y_train, count_y_test = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "    \n",
    "    # Train and predict\n",
    "    print(f\"-------------------------MIN_DF={min_df}-------------------------\")\n",
    "    print(\"Trained with tfidf:\")\n",
    "    print(classification_report(tfidf_y_test, MultinomialNB().fit(tfidf_train, tfidf_y_train).predict(tfidf_test)), '\\n')\n",
    "    print(\"Trained with bow:\")\n",
    "    print(classification_report(count_y_test, MultinomialNB().fit(count_train, count_y_train).predict(count_test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comparing the accuracy of both settings, they seem to be very similar. For every frequency threshold the results are almost identical.*\n",
    "\n",
    "*For min_df = 2 the accuracy is 0.85 for TF-IDF compared to 0.84 for Bag of words.*\n",
    "\n",
    "*For min_df = 5 the accuracy is 0.84 for TF-IDF compared to 0.85 for Bag of words.*\n",
    "\n",
    "*For min_df = 10 the accuracy is 0.84 for both settings.*\n",
    "\n",
    "*Looking at these results we can also see that the frequency threshold does not really affect the scores. By increasing the min_df, more terms should be ignored that appear too infrequent. But by removing these terms, the accuracy does not increase. But a higher frequency threshold may be beneficial still by reducing the dimensionality of the imput vector without negatively impacting performance.*\n",
    "\n",
    "*The reason for why a higher frequency threshold does not reduce accuracy might be because the words are still considered relatively infrequent and so the model is not able to learn an association between them and the sentiment of the tweet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airline_vec = CountVectorizer(min_df=min_df, tokenizer=nltk.word_tokenize, stop_words=stopwords.words('english'))\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "data, _, labels, _ = train_test_split(airline_counts, airline_tweets_train.target, test_size=0.2)\n",
    "clf = MultinomialNB().fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1529.0 @\n",
      "0 1389.0 united\n",
      "0 1249.0 .\n",
      "0 416.0 ``\n",
      "0 409.0 flight\n",
      "0 401.0 ?\n",
      "0 371.0 !\n",
      "0 325.0 #\n",
      "0 222.0 n't\n",
      "0 160.0 ''\n",
      "0 139.0 's\n",
      "0 114.0 :\n",
      "0 111.0 service\n",
      "0 108.0 virginamerica\n",
      "0 99.0 get\n",
      "0 99.0 cancelled\n",
      "0 92.0 delayed\n",
      "0 91.0 bag\n",
      "0 89.0 customer\n",
      "0 86.0 time\n",
      "0 84.0 plane\n",
      "0 74.0 ...\n",
      "0 74.0 'm\n",
      "0 73.0 http\n",
      "0 73.0 -\n",
      "0 69.0 hours\n",
      "0 68.0 gate\n",
      "0 65.0 ;\n",
      "0 64.0 hour\n",
      "0 61.0 still\n",
      "0 59.0 airline\n",
      "0 58.0 late\n",
      "0 58.0 help\n",
      "0 57.0 would\n",
      "0 57.0 &\n",
      "0 56.0 ca\n",
      "0 56.0 2\n",
      "0 55.0 flights\n",
      "0 52.0 amp\n",
      "0 51.0 worst\n",
      "0 51.0 like\n",
      "0 50.0 one\n",
      "0 50.0 $\n",
      "0 48.0 flightled\n",
      "0 47.0 delay\n",
      "0 46.0 've\n",
      "0 45.0 waiting\n",
      "0 45.0 never\n",
      "0 43.0 us\n",
      "0 43.0 3\n",
      "0 43.0 (\n",
      "0 40.0 really\n",
      "0 40.0 lost\n",
      "0 40.0 ever\n",
      "0 40.0 )\n",
      "0 39.0 back\n",
      "0 38.0 wait\n",
      "0 37.0 u\n",
      "0 37.0 last\n",
      "0 37.0 check\n",
      "0 36.0 seat\n",
      "0 36.0 due\n",
      "0 36.0 another\n",
      "0 35.0 fly\n",
      "0 35.0 day\n",
      "0 33.0 seats\n",
      "0 33.0 people\n",
      "0 33.0 luggage\n",
      "0 33.0 bags\n",
      "0 32.0 ticket\n",
      "0 32.0 thanks\n",
      "0 32.0 could\n",
      "0 32.0 airport\n",
      "0 31.0 hold\n",
      "0 31.0 guys\n",
      "0 31.0 even\n",
      "0 30.0 problems\n",
      "0 30.0 need\n",
      "0 29.0 trying\n",
      "0 29.0 staff\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1394.0 @\n",
      "1 523.0 ?\n",
      "1 521.0 .\n",
      "1 304.0 jetblue\n",
      "1 266.0 :\n",
      "1 261.0 united\n",
      "1 259.0 southwestair\n",
      "1 249.0 #\n",
      "1 244.0 flight\n",
      "1 240.0 ``\n",
      "1 186.0 americanair\n",
      "1 169.0 !\n",
      "1 168.0 http\n",
      "1 164.0 usairways\n",
      "1 134.0 's\n",
      "1 83.0 get\n",
      "1 73.0 ''\n",
      "1 72.0 -\n",
      "1 71.0 virginamerica\n",
      "1 65.0 flights\n",
      "1 64.0 please\n",
      "1 62.0 help\n",
      "1 58.0 )\n",
      "1 54.0 n't\n",
      "1 52.0 need\n",
      "1 52.0 ...\n",
      "1 48.0 (\n",
      "1 45.0 ;\n",
      "1 43.0 us\n",
      "1 40.0 dm\n",
      "1 39.0 would\n",
      "1 39.0 tomorrow\n",
      "1 37.0 know\n",
      "1 37.0 &\n",
      "1 35.0 “\n",
      "1 35.0 way\n",
      "1 35.0 thanks\n",
      "1 35.0 fleet\n",
      "1 35.0 fleek\n",
      "1 34.0 change\n",
      "1 33.0 hi\n",
      "1 31.0 ”\n",
      "1 31.0 like\n",
      "1 31.0 could\n",
      "1 31.0 'm\n",
      "1 29.0 flying\n",
      "1 28.0 cancelled\n",
      "1 28.0 amp\n",
      "1 27.0 fly\n",
      "1 26.0 travel\n",
      "1 26.0 time\n",
      "1 26.0 one\n",
      "1 26.0 number\n",
      "1 25.0 see\n",
      "1 25.0 new\n",
      "1 25.0 check\n",
      "1 23.0 ticket\n",
      "1 22.0 today\n",
      "1 22.0 airport\n",
      "1 21.0 destinationdragons\n",
      "1 21.0 2\n",
      "1 20.0 rt\n",
      "1 20.0 make\n",
      "1 20.0 going\n",
      "1 20.0 chance\n",
      "1 19.0 use\n",
      "1 19.0 next\n",
      "1 19.0 go\n",
      "1 19.0 booked\n",
      "1 19.0 back\n",
      "1 18.0 sent\n",
      "1 18.0 guys\n",
      "1 18.0 gate\n",
      "1 18.0 follow\n",
      "1 18.0 first\n",
      "1 17.0 trying\n",
      "1 17.0 tickets\n",
      "1 17.0 start\n",
      "1 17.0 seat\n",
      "1 17.0 reservation\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1321.0 @\n",
      "2 1038.0 !\n",
      "2 751.0 .\n",
      "2 304.0 southwestair\n",
      "2 296.0 thanks\n",
      "2 290.0 #\n",
      "2 283.0 jetblue\n",
      "2 252.0 united\n",
      "2 244.0 ``\n",
      "2 241.0 thank\n",
      "2 182.0 flight\n",
      "2 165.0 :\n",
      "2 162.0 americanair\n",
      "2 138.0 usairways\n",
      "2 136.0 great\n",
      "2 96.0 service\n",
      "2 92.0 )\n",
      "2 87.0 virginamerica\n",
      "2 67.0 much\n",
      "2 67.0 http\n",
      "2 65.0 love\n",
      "2 65.0 customer\n",
      "2 64.0 guys\n",
      "2 60.0 best\n",
      "2 57.0 awesome\n",
      "2 57.0 's\n",
      "2 56.0 ;\n",
      "2 51.0 -\n",
      "2 48.0 good\n",
      "2 45.0 time\n",
      "2 44.0 airline\n",
      "2 43.0 amazing\n",
      "2 42.0 got\n",
      "2 41.0 us\n",
      "2 41.0 crew\n",
      "2 40.0 &\n",
      "2 39.0 n't\n",
      "2 36.0 fly\n",
      "2 35.0 today\n",
      "2 35.0 help\n",
      "2 35.0 get\n",
      "2 34.0 ''\n",
      "2 33.0 made\n",
      "2 33.0 amp\n",
      "2 32.0 response\n",
      "2 30.0 flying\n",
      "2 29.0 home\n",
      "2 29.0 appreciate\n",
      "2 29.0 ...\n",
      "2 26.0 see\n",
      "2 26.0 day\n",
      "2 26.0 back\n",
      "2 26.0 ?\n",
      "2 25.0 work\n",
      "2 25.0 like\n",
      "2 24.0 u\n",
      "2 24.0 nice\n",
      "2 24.0 gate\n",
      "2 24.0 'm\n",
      "2 23.0 team\n",
      "2 23.0 new\n",
      "2 23.0 know\n",
      "2 23.0 first\n",
      "2 23.0 (\n",
      "2 22.0 well\n",
      "2 22.0 tonight\n",
      "2 22.0 ever\n",
      "2 21.0 would\n",
      "2 21.0 please\n",
      "2 21.0 're\n",
      "2 20.0 yes\n",
      "2 20.0 southwest\n",
      "2 20.0 quick\n",
      "2 20.0 helpful\n",
      "2 20.0 getting\n",
      "2 19.0 plane\n",
      "2 19.0 job\n",
      "2 19.0 class\n",
      "2 18.0 staff\n",
      "2 18.0 happy\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer, classifier, n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Expected features for the negative class are cancelled, delayed, worst, last etc since they are clearly negative. There are a lot of neutral words listed which are a bit unexpected such as united, flight, service.*\n",
    "\n",
    "*Expected features for the neutral class are words like names such as jetblue, southwestair, americair and tomorrow, know. We did not expect 'help' and 'please' to be that highly ranked in the neutral list.*\n",
    "\n",
    "*For the positive list we see positive words like thanks, great, love etc in the upper part of the list. It is unexpected however that names such as the airlines are ranked really high (southwestair and jetblue higher than thanks).*\n",
    "\n",
    "*The tweets are about airlines so a high occurence of names of the airline companies are expected. Maybe deleting these would improve the ranking of the model. On the other hand, certain airlines may generally have worse or better client satisfaction and may be correlated with the sentiment.*\n",
    "\n",
    "*To further improve the model we would probably delete the standard make-up of the tweets such as: @, # and http. We would also remove stop-words. We would keep negations since negation handling plays an important role in classification. An example would be n't which is vital in sentences.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
